---
title: "Analysis"
output: html_notebook
---

We applied a stacked model consists of base learners (Lasso, gaussain processes and XG-Boost) stacked with a Random forest.

## load libraries

```{r}
library(caret)
library(caretEnsemble)
library(tidyverse)

source("200_optimization.R")
```

## Load the data

```{r}
path <- getwd()
dataset_final <- read.csv(paste(path, "data_set_pheno_resd_auto.csv", sep = "/"))
```


Split the data (idential to the previous split)

```{r}
set.seed(1234)

dim(dataset_final)
s1 <- sample(1:nrow(dataset_final), 500)

pheno_no <- grep("pheno_dpw", colnames(dataset_final))

trainx <- as.matrix(dataset1[-s1, -pheno_no])
testx <- as.matrix(dataset1[s1, -pheno_no])
trainy <- as.matrix(dataset1$Phenotype[-s1])
testy <- as.matrix(dataset1$Phenotype[s1])
```

We fit each base leaner separately to optimizte the hyperparameters using Bayesian Optimization

## Gaussian Process

```{r}
## Define the resampling method
ctrl <- trainControl(method = "repeatedcv", repeats = 5, search = "random")
## Use this function to optimize the model. The two parameters are 
## evaluated on the log scale given their range and scope. 

gbm_rand <- caret::train(
  trainx,
  as.numeric(trainy),
  method = "gaussprRadial",
  metric = "RMSE",
  tunelength = 20,
  trControl = ctrl,
  verbose = FALSE
  ) 

## Define the bounds of the search. 
lower_bounds <- c(Sigma = 0)
upper_bounds <- c(Sigma = 1)
bounds <- list(Sigma = c(lower_bounds[1], 
                         upper_bounds[1]))

## Create a grid of values as the input into the BO code
initial_grid <- gbm_rand$results[, c("sigma", "RMSE")]
initial_grid$RMSE <- -initial_grid$RMSE
names(initial_grid) <- c("Sigma", "Value")


ba_gau <- BayesianOptimization(
  gau_fit_bayes,
  bounds = bounds,
  init_grid_dt = initial_grid,
  init_points = 10,
  n_iter = 30,
  acq = "ucb",
  kappa = 1,
  eps = 0.0,
  verbose = TRUE
  )

ctrl1 <- trainControl(method = "repeatedcv",
                      repeats = 5)

final_gau <- caret::train(
  trainx,
  as.numeric(trainy),
  method = "gaussprRadial",
  tuneGrid = data.frame(sigma =
  ba_gau$Best_Par["Sigma"]),
  metric = "RMSE",
  trControl = ctrl1
  )

pred_gau <- predict(final_gau, testx)

mean((pred_gau - testy)^2)
```


## Lasso

```{r}
## Define the resampling method

lasso_rand <- caret::train(
  trainx,
  as.numeric(trainy),
  method = "glmnet",
  metric = "RMSE",
  tunelength = 20,
  trControl = ctrl,
  verbose = FALSE
  ) 

## Define the bounds of the search. 
lower_bounds <- c(lambda = 0, alpha = 0)
upper_bounds <- c(lambda = 1, alpha = 1)
bounds <- list(
  lambda = c(lower_bounds[1], upper_bounds[1]),
  alpha =  c(lower_bounds[2], upper_bounds[2])
  )

## Create a grid of values as the input into the BO code
initial_grid <- lasso_rand$results[, c("lambda", "alpha", "RMSE")]
initial_grid$RMSE <- -initial_grid$RMSE
names(initial_grid) <- c("Lambda", "Alpha", "Value")


ba_lasso <- BayesianOptimization(
  lasso_fit_bayes,
  bounds = bounds,
  init_grid_dt = initial_grid,
  init_points = 10,
  n_iter = 30,
  acq = "ucb",
  kappa = 1,
  eps = 0.0,
  verbose = TRUE
  )


final_lasso <- caret::train(
  trainx,
  as.numeric(trainy),
  method = "glmnet",
  tuneGrid = data.frame(lambda = ba_lasso$Best_Par["lambda"],
  alpha = ba_lasso$Best_Par["Alpha"]),
  metric = "RMSE",
  trControl = ctrl1
  )

pred_lasso <- predict(final_lasso, testx)

mean((pred_lasso - testy)^2)
```


## XG-Boosting

```{r}
## Define the resampling method

xgboost_rand <- caret::train(trainx, as.numeric(trainy),
                             method="xgbTree",
                             metric="RMSE",
                             tunelength = 20,
                             trControl=ctrl,
                             verbose=FALSE
) 


## Define the bounds of the search. 
lower_bounds <- c(
  nrounds = 50,
  eta = 0,
  max_depth = 1,
  min_child_weight = 1,
  subsample = 0
  )

upper_bounds <- c(
  nrounds = 1000,
  eta = 1,
  max_depth = 10,
  min_child_weight = 100,
  subsample = 1
  )

bounds <- list(
  nrounds = c(lower_bounds[1], upper_bounds[1]),
  eta =  c(lower_bounds[2], upper_bounds[2]),
  max_depth = c(lower_bounds[3], upper_bounds[3]),
  min_child_weight =  c(lower_bounds[4], upper_bounds[4]),
  subsample =  c(lower_bounds[5], upper_bounds[5])
  )
                              

## Create a grid of values as the input into the BO code
initial_grid <- xgboost_rand$results[, c("nrounds",
                                         "eta",
                                         "max_depth",
                                         "min_child_weight",
                                         "subsample",
                                         "RMSE")]

initial_grid$RMSE <- -initial_grid$RMSE
names(initial_grid) <- c("nrounds",
                         "eta",
                         "max_depth",
                         "min_child_weight",
                         "subsample",
                         "Value")


ba_xgboost <- BayesianOptimization(
  xgboost_fit_bayes,
  bounds = bounds,
  init_grid_dt = initial_grid,
  init_points = 10,
  n_iter = 30,
  acq = "ucb",
  kappa = 1,
  eps = 0.0,
  verbose = TRUE
  )


final_xgboost <- train(
  trainx,
  as.numeric(trainy),
  method = "gaussprRadial",
  tuneGrid = data.frame(
  nrounds = ba_xgboost$Best_Par["nrounds"],
  eta = ba_xgboost$Best_Par["eta"],
  max_depth = ba_xgboost$Best_Par["max_depth"],
  min_child_weight = ba_xgboost$Best_Par["min_child_weight"],
  subsample = ba_xgboost$Best_Par["subsample"],
  colsample_bytree = 0.3),
  metric = "RMSE",
  trControl = ctrl1
  )

pred_xgboost <- predict(final_xgboost, testx)

mean((pred_xgboost - testy)^2)
```

Combining the model into an ensemble

```{r}

model_list_big <- caretList(
  trainx,
  as.numeric(trainy),
  trControl = ctrl1,
  methodList = c("glmnet", "gaussprRadial", "xgbTree"),
  metric = "RMSE",
  tuneList = list(glmnet = caretModelSpec(
    method = "glmnet",
    tuneGrid = data.frame(
      .alpha = ba_lasso$Best_Par["alpha"],
      .lambdaba_lasso$Best_Par["lambda"])),
    gauss = caretModelSpec(
    method = "gaussprRadial",
    tuneGrid = data.frame(
      .sigma = ba_gau$Best_Par["Sigma"])),
    xg = caretModelSpec(
    method = "xgbTree",
    tuneGrid = data.frame(
      .nrounds = ba_xgboost$Best_Par["nrounds"],
      .eta = ba_xgboost$Best_Par["eta"],
      .max_depth = ba_xgboost$Best_Par["max_depth"],
      .min_child_weight = ba_xgboost$Best_Par["min_child_weight"],
      .subsample = ba_xgboost$Best_Par["subsample"],
      .colsample_bytree = 0.3))
    )
  )



stack_model <- caretStack(
  model_list_big,
  method = "rf",
  metric = "RMSE",
  trControl = crtl1
  )

pred_stack <- predict(stack_model, testx)

postResample(pred = pred_stack, obs = testy)

```

The hyperparameter optimization is done using bayesian optimization method.